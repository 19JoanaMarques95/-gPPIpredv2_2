{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zynQrTzuFRe4"
      },
      "outputs": [],
      "source": [
        "#@title mount drive\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title install dependencies\n",
        "!pip install torch_geometric\n",
        "!pip install torch_scatter\n",
        "!pip install torch_sparse\n",
        "!pip install Biopython\n",
        "!pip install gradio"
      ],
      "metadata": {
        "id": "vYYyk39lFZJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch_geometric.data import Data, Batch\n",
        "from torch_geometric.utils import add_self_loops\n",
        "# --- 1. SCALED AA PHYSICS (Change A) ---\n",
        "# Values are normalized: (x - min) / (max - min) roughly.\n",
        "# This prevents Volume from drowning out everything else.\n",
        "AA_PHYSICS_5D_SCALED = {\n",
        "    'A': [0.70, 0.17, 0.11, 0.40, 0.00], 'R': [0.00, 0.67, 0.71, 1.00, 1.00],\n",
        "    'N': [0.11, 0.32, 0.33, 0.33, 0.00], 'D': [0.11, 0.30, 0.26, 0.00, 0.31],\n",
        "    'C': [0.78, 0.29, 0.31, 0.29, 0.67], 'Q': [0.11, 0.50, 0.44, 0.36, 0.00],\n",
        "    'E': [0.11, 0.47, 0.37, 0.06, 0.34], 'G': [0.46, 0.00, 0.00, 0.40, 0.00],\n",
        "    'H': [0.14, 0.55, 0.56, 0.60, 0.48], 'I': [1.00, 0.64, 0.45, 0.41, 0.00],\n",
        "    'L': [0.92, 0.64, 0.45, 0.40, 0.00], 'K': [0.07, 0.65, 0.54, 0.87, 0.84],\n",
        "    'M': [0.66, 0.61, 0.54, 0.37, 0.00], 'F': [0.81, 0.77, 0.72, 0.34, 0.00],\n",
        "    'P': [0.32, 0.31, 0.32, 0.44, 0.00], 'S': [0.41, 0.17, 0.15, 0.36, 0.00],\n",
        "    'T': [0.42, 0.33, 0.26, 0.35, 0.00], 'W': [0.40, 1.00, 1.00, 0.39, 0.00],\n",
        "    'Y': [0.36, 0.79, 0.73, 0.36, 0.81], 'V': [0.97, 0.48, 0.34, 0.40, 0.00],\n",
        "    'X': [0.00, 0.00, 0.00, 0.00, 0.00]\n",
        "}\n",
        "\n",
        "class PPIDatasetV3(Dataset):\n",
        "    def __init__(self, csv_path, id_map, base_path):\n",
        "        self.df = pd.read_csv(csv_path)\n",
        "        self.id_map = id_map\n",
        "        self.base_path = base_path\n",
        "        self.cache = {}\n",
        "\n",
        "    def _get_graph(self, p_id):\n",
        "        p_id = str(p_id)\n",
        "        if p_id in self.cache: return self.cache[p_id]\n",
        "\n",
        "        info = self.id_map[p_id]\n",
        "        adj = np.load(os.path.join(self.base_path, info['adj_file'])).astype(np.float32)\n",
        "        seq = np.load(os.path.join(self.base_path, info['seq_file']), allow_pickle=True)[0]\n",
        "\n",
        "        # Build 5D features using the SCALED dictionary\n",
        "        x = torch.tensor([AA_PHYSICS_5D_SCALED.get(a, AA_PHYSICS_5D_SCALED['X']) for a in seq], dtype=torch.float32)\n",
        "\n",
        "        edge_index = torch.from_numpy(np.array(np.nonzero(adj))).to(torch.long)\n",
        "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
        "\n",
        "        graph = Data(x=x, edge_index=edge_index)\n",
        "        self.cache[p_id] = graph\n",
        "        return graph\n",
        "\n",
        "    def __len__(self): return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        return self._get_graph(row['InteractorA']), self._get_graph(row['InteractorB']), torch.tensor([float(row['Label'])])\n",
        "\n",
        "def collate(batch):\n",
        "    p1, p2, y = zip(*batch)\n",
        "    return Batch.from_data_list(p1), Batch.from_data_list(p2), torch.stack(y)"
      ],
      "metadata": {
        "id": "plXGtRPLFmqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "tkDjbo6dFwWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch_geometric.data import Data, Batch\n",
        "from torch_geometric.utils import add_self_loops\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "BASE_PATH = '/content/drive/MyDrive/gppipredv2_2/'\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# --- 1. SCALED AA PHYSICS ---\n",
        "AA_PHYSICS_5D_SCALED = {\n",
        "    'A': [0.70, 0.17, 0.11, 0.40, 0.00], 'R': [0.00, 0.67, 0.71, 1.00, 1.00],\n",
        "    'N': [0.11, 0.32, 0.33, 0.33, 0.00], 'D': [0.11, 0.30, 0.26, 0.00, 0.31],\n",
        "    'C': [0.78, 0.29, 0.31, 0.29, 0.67], 'Q': [0.11, 0.50, 0.44, 0.36, 0.00],\n",
        "    'E': [0.11, 0.47, 0.37, 0.06, 0.34], 'G': [0.46, 0.00, 0.00, 0.40, 0.00],\n",
        "    'H': [0.14, 0.55, 0.56, 0.60, 0.48], 'I': [1.00, 0.64, 0.45, 0.41, 0.00],\n",
        "    'L': [0.92, 0.64, 0.45, 0.40, 0.00], 'K': [0.07, 0.65, 0.54, 0.87, 0.84],\n",
        "    'M': [0.66, 0.61, 0.54, 0.37, 0.00], 'F': [0.81, 0.77, 0.72, 0.34, 0.00],\n",
        "    'P': [0.32, 0.31, 0.32, 0.44, 0.00], 'S': [0.41, 0.17, 0.15, 0.36, 0.00],\n",
        "    'T': [0.42, 0.33, 0.26, 0.35, 0.00], 'W': [0.40, 1.00, 1.00, 0.39, 0.00],\n",
        "    'Y': [0.36, 0.79, 0.73, 0.36, 0.81], 'V': [0.97, 0.48, 0.34, 0.40, 0.00],\n",
        "    'X': [0.00, 0.00, 0.00, 0.00, 0.00]\n",
        "}\n",
        "\n",
        "# --- 2. DATASET & COLLATE ---\n",
        "class PPIDatasetV3(Dataset):\n",
        "    def __init__(self, csv_path, id_map, base_path):\n",
        "        self.df = pd.read_csv(csv_path)\n",
        "        self.id_map = id_map\n",
        "        self.base_path = base_path\n",
        "        self.cache = {}\n",
        "\n",
        "    def _get_graph(self, p_id):\n",
        "        p_id = str(p_id)\n",
        "        if p_id in self.cache: return self.cache[p_id]\n",
        "\n",
        "        info = self.id_map[p_id]\n",
        "        adj = np.load(os.path.join(self.base_path, info['adj_file'])).astype(np.float32)\n",
        "        seq = np.load(os.path.join(self.base_path, info['seq_file']), allow_pickle=True)[0]\n",
        "\n",
        "        x = torch.tensor([AA_PHYSICS_5D_SCALED.get(a, AA_PHYSICS_5D_SCALED['X']) for a in seq], dtype=torch.float32)\n",
        "        edge_index = torch.from_numpy(np.array(np.nonzero(adj))).to(torch.long)\n",
        "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
        "\n",
        "        graph = Data(x=x, edge_index=edge_index)\n",
        "        self.cache[p_id] = graph\n",
        "        return graph\n",
        "\n",
        "    def __len__(self): return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        return self._get_graph(row['InteractorA']), self._get_graph(row['InteractorB']), torch.tensor([float(row['Label'])])\n",
        "\n",
        "def collate(batch):\n",
        "    p1, p2, y = zip(*batch)\n",
        "    return Batch.from_data_list(p1), Batch.from_data_list(p2), torch.stack(y)\n",
        "\n",
        "# --- 3. INITIALIZATION ---\n",
        "# Update these paths to your current working files\n",
        "# --- UPDATE THESE PATHS IN YOUR TRAINING CELL ---\n",
        "TRAIN_CSV = \"/content/drive/MyDrive/gppipredv2_2/train_subset_90.csv\"\n",
        "VAL_CSV = \"/content/drive/MyDrive/gppipredv2_2/internal_sanity_val.csv\"\n",
        "\n",
        "\n",
        "\n",
        "train_ds = PPIDatasetV3(TRAIN_CSV, master_map, BASE_PATH)\n",
        "val_ds = PPIDatasetV3(VAL_CSV, master_map, BASE_PATH)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, collate_fn=collate)\n",
        "val_loader = DataLoader(val_ds, batch_size=128, shuffle=False, collate_fn=collate)\n",
        "\n",
        "model = SiameseGAT_v3(in_channels=5).to(DEVICE)\n",
        "\n",
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        nn.init.xavier_uniform_(m.weight)\n",
        "        if m.bias is not None: nn.init.zeros_(m.bias)\n",
        "\n",
        "model.apply(init_weights)\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.05)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "best_mcc = 0.0\n",
        "print(\"üöÄ Fresh Training Session Started. Goal: Stable Positive MCC.\")\n",
        "\n",
        "# --- 4. TRAINING LOOP ---\n",
        "for epoch in range(1, 31):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch} [Train]\")\n",
        "\n",
        "    for g1, g2, y in pbar:\n",
        "        g1, g2, y = g1.to(DEVICE), g2.to(DEVICE), y.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(g1, g2)\n",
        "        loss = criterion(logits, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
        "\n",
        "    # --- VALIDATION ---\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for g1, g2, y in tqdm(val_loader, desc=f\"Epoch {epoch} [Val]\"):\n",
        "            g1, g2, y = g1.to(DEVICE), g2.to(DEVICE), y.to(DEVICE)\n",
        "            logits = model(g1, g2)\n",
        "            # LOGITS > 0.0 is the correct threshold for BCEWithLogitsLoss\n",
        "            preds = (logits > 0.0).float()\n",
        "            all_preds.extend(preds.cpu().numpy().flatten())\n",
        "            all_labels.extend(y.cpu().numpy().flatten())\n",
        "\n",
        "    val_mcc = matthews_corrcoef(all_labels, all_preds)\n",
        "    avg_loss = epoch_loss / len(train_loader)\n",
        "\n",
        "    print(f\"üìà Epoch {epoch} | Loss: {avg_loss:.4f} | Val MCC: {val_mcc:.4f}\")\n",
        "\n",
        "    if val_mcc > best_mcc:\n",
        "        best_mcc = val_mcc\n",
        "        torch.save(model.state_dict(), f\"{BASE_PATH}/best_siamese_v3_boat.pt\")\n",
        "        print(f\"‚≠ê New Best MCC: {best_mcc:.4f}\")"
      ],
      "metadata": {
        "id": "g5908ofJF06D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Validation"
      ],
      "metadata": {
        "id": "NIvZDZ9YF77R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch_geometric.data import Data, Batch\n",
        "from torch_geometric.utils import add_self_loops\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.metrics import matthews_corrcoef, confusion_matrix, roc_auc_score, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# --- 1. LOCAL DATA PREPARATION ---\n",
        "LOCAL_DATA_DIR = \"/content/local_graphs\"\n",
        "os.makedirs(LOCAL_DATA_DIR, exist_ok=True)\n",
        "\n",
        "print(\"üöö Copying files to local disk (this speed up is necessary for 72k pairs)...\")\n",
        "# Copy only npy files from your Drive folder to the local VM\n",
        "!cp -r /content/drive/MyDrive/gppipredv2_2/validation_protein_features/*.npy {LOCAL_DATA_DIR}/\n",
        "print(\"‚úÖ Files localized.\")\n",
        "\n",
        "# --- 2. DATASET DEFINITION (RAM-SAFE) ---\n",
        "class PPIDatasetV3_Final(Dataset):\n",
        "    def __init__(self, df, id_map, data_folder):\n",
        "        self.df = df\n",
        "        self.id_map = id_map\n",
        "        self.data_folder = data_folder\n",
        "        # We removed self.cache to avoid RAM crashes on large datasets\n",
        "\n",
        "    def _get_graph(self, p_id):\n",
        "        p_id = str(p_id)\n",
        "        info = self.id_map[p_id]\n",
        "\n",
        "        # Load directly from LOCAL disk\n",
        "        adj = np.load(os.path.join(self.data_folder, info['adj_file'])).astype(np.float32)\n",
        "        seq = np.load(os.path.join(self.data_folder, info['seq_file']), allow_pickle=True)[0]\n",
        "\n",
        "        x = torch.tensor([AA_PHYSICS_5D_SCALED.get(a, AA_PHYSICS_5D_SCALED['X']) for a in seq], dtype=torch.float32)\n",
        "        edge_index = torch.from_numpy(np.array(np.nonzero(adj))).to(torch.long)\n",
        "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
        "\n",
        "        return Data(x=x, edge_index=edge_index)\n",
        "\n",
        "    def __len__(self): return len(self.df)\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        return self._get_graph(row['InteractorA']), self._get_graph(row['InteractorB']), torch.tensor([float(row['Label'])])\n",
        "\n",
        "def collate(batch):\n",
        "    p1, p2, y = zip(*batch)\n",
        "    return Batch.from_data_list(p1), Batch.from_data_list(p2), torch.stack(y)\n",
        "\n",
        "# --- 3. RUN VALIDATION ---\n",
        "# Ensure variables from earlier (df_ready, master_map, model) are still in memory\n",
        "val_ds = PPIDatasetV3_Final(df_ready, master_map, LOCAL_DATA_DIR)\n",
        "val_loader = DataLoader(val_ds, batch_size=128, shuffle=False, collate_fn=collate, num_workers=2)\n",
        "\n",
        "model.eval()\n",
        "all_logits, all_labels = [], []\n",
        "\n",
        "print(\"üî¨ Starting Validation Loop...\")\n",
        "with torch.no_grad():\n",
        "    for g1, g2, y in tqdm(val_loader, desc=\"Validating 72k Pairs\"):\n",
        "        logits = model(g1.to(DEVICE), g2.to(DEVICE))\n",
        "        all_logits.extend(logits.cpu().numpy().flatten())\n",
        "        all_labels.extend(y.cpu().numpy().flatten())\n",
        "\n",
        "# --- 4. CALCULATE & PLOT ---\n",
        "probs = 1 - (1 / (1 + np.exp(-np.array(all_logits)))) # Flipped Boat Logic\n",
        "preds = (probs > 0.5).astype(float)\n",
        "\n",
        "mcc = matthews_corrcoef(all_labels, preds)\n",
        "auc = roc_auc_score(all_labels, probs)\n",
        "print(f\"\\nüìä FINAL RESULTS: MCC={mcc:.4f} | AUC={auc:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "cm = confusion_matrix(all_labels, preds)\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title(f'Final Validation (n=72k) | MCC: {mcc:.4f}')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "b0ijh4LxF_1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_mcc = -1\n",
        "best_threshold = 0.5\n",
        "\n",
        "# Test different thresholds to maximize MCC\n",
        "for t in np.arange(0.1, 0.9, 0.05):\n",
        "    t_preds = (probs > t).astype(float)\n",
        "    t_mcc = matthews_corrcoef(all_labels, t_preds)\n",
        "    if t_mcc > best_mcc:\n",
        "        best_mcc = t_mcc\n",
        "        best_threshold = t\n",
        "\n",
        "print(f\"üéØ Optimal Threshold: {best_threshold:.2f}\")\n",
        "print(f\"üöÄ Optimized MCC: {best_mcc:.4f}\")"
      ],
      "metadata": {
        "id": "9xp4HwgaXL-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
        "\n",
        "def generate_final_figures(labels, probabilities, threshold=0.85):\n",
        "    # Calculate predictions based on optimized threshold\n",
        "    preds = (probabilities > threshold).astype(float)\n",
        "\n",
        "    # Setup Figure\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "    plt.rcParams.update({'font.size': 12, 'font.family': 'sans-serif'})\n",
        "\n",
        "    # --- Panel A: ROC Curve ---\n",
        "    fpr, tpr, _ = roc_curve(labels, probabilities)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    axes[0].plot(fpr, tpr, color='#1f77b4', lw=3, label=f'ROC AUC = {roc_auc:.4f}')\n",
        "    axes[0].plot([0, 1], [0, 1], color='grey', lw=1, linestyle='--')\n",
        "    axes[0].set_xlim([0.0, 1.0])\n",
        "    axes[0].set_ylim([0.0, 1.05])\n",
        "    axes[0].set_xlabel('False Positive Rate (1 - Specificity)')\n",
        "    axes[0].set_ylabel('True Positive Rate (Sensitivity)')\n",
        "    axes[0].set_title('A. Receiver Operating Characteristic')\n",
        "    axes[0].legend(loc=\"lower right\", frameon=True)\n",
        "    axes[0].grid(alpha=0.2)\n",
        "\n",
        "    # --- Panel B: Confusion Matrix ---\n",
        "    cm = confusion_matrix(labels, preds)\n",
        "    # Normalized for percentage-based view\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1], cbar=False)\n",
        "\n",
        "    axes[1].set_title(f'B. Confusion Matrix (Threshold = {threshold})')\n",
        "    axes[1].set_xlabel('Predicted Label')\n",
        "    axes[1].set_ylabel('True Label')\n",
        "    axes[1].set_xticklabels(['Non-Binder', 'Binder'])\n",
        "    axes[1].set_yticklabels(['Non-Binder', 'Binder'])\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save files\n",
        "    plt.savefig('Final_GATv3_Metrics.pdf', dpi=300, bbox_inches='tight')\n",
        "    plt.savefig('Final_GATv3_Metrics.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "# Run the plot\n",
        "generate_final_figures(all_labels, probs, threshold=0.85)"
      ],
      "metadata": {
        "id": "3w40ZceFXTeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, f1_score, precision_score, recall_score\n",
        "\n",
        "# 1. Calculate metrics at the optimal 0.85 threshold\n",
        "final_preds = (probs > 0.85).astype(float)\n",
        "\n",
        "# 2. Compute components\n",
        "precision = precision_score(all_labels, final_preds)\n",
        "recall = recall_score(all_labels, final_preds)\n",
        "f1 = f1_score(all_labels, final_preds)\n",
        "\n",
        "# 3. Create a summary dataframe for the paper\n",
        "metrics_data = {\n",
        "    \"Metric\": [\"ROC-AUC\", \"MCC\", \"Precision\", \"Recall (Sensitivity)\", \"F1-Score\", \"Accuracy\"],\n",
        "    \"Value\": [\n",
        "        f\"{roc_auc_score(all_labels, probs):.4f}\",\n",
        "        f\"{matthews_corrcoef(all_labels, final_preds):.4f}\",\n",
        "        f\"{precision:.4f}\",\n",
        "        f\"{recall:.4f}\",\n",
        "        f\"{f1:.4f}\",\n",
        "        f\"{accuracy_score(all_labels, final_preds):.4f}\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "metrics_df = pd.DataFrame(metrics_data)\n",
        "\n",
        "# Print as a nice Markdown table\n",
        "print(\"### üìä Final Validation Performance (Threshold = 0.85)\")\n",
        "print(metrics_df.to_markdown(index=False))\n",
        "\n",
        "# Optional: Print the full per-class report\n",
        "print(\"\\n### üß¨ Detailed Per-Class Report\")\n",
        "print(classification_report(all_labels, final_preds, target_names=['Non-Binder (0)', 'Binder (1)']))"
      ],
      "metadata": {
        "id": "vxBLFHoJXWTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "thresholds = np.arange(0.1, 0.95, 0.02)\n",
        "mccs = [matthews_corrcoef(all_labels, (probs > t).astype(float)) for t in thresholds]\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(thresholds, mccs, color='#e67e22', lw=2)\n",
        "plt.axvline(0.85, color='red', linestyle='--', label='Optimum (0.85)')\n",
        "plt.title('MCC vs. Classification Threshold')\n",
        "plt.xlabel('Threshold')\n",
        "plt.ylabel('Matthews Correlation Coefficient')\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.savefig('MCC_Threshold_Optimization.png', dpi=300)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LgTIcwcEXZVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_precision_recall_vs_threshold(labels, probabilities, optimal_t=0.85):\n",
        "    # Calculate precision and recall for all possible thresholds\n",
        "    precisions, recalls, thresholds = precision_recall_curve(labels, probabilities)\n",
        "\n",
        "    # Setup the figure\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.rcParams.update({'font.size': 12})\n",
        "\n",
        "    # Plot Precision and Recall\n",
        "    # Note: precision and recall have one more element than thresholds, so we slice them\n",
        "    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\", linewidth=2)\n",
        "    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall (Sensitivity)\", linewidth=2)\n",
        "\n",
        "    # Highlight the 0.85 threshold\n",
        "    plt.axvline(x=optimal_t, color='red', linestyle=':', alpha=0.7)\n",
        "    plt.annotate(f'Selected Threshold: {optimal_t}',\n",
        "                 xy=(optimal_t, 0.65), xytext=(optimal_t-0.3, 0.4),\n",
        "                 arrowprops=dict(facecolor='black', shrink=0.05, width=1, headwidth=8))\n",
        "\n",
        "    # Add labels and styling\n",
        "    plt.title('Precision and Recall vs. Decision Threshold')\n",
        "    plt.xlabel('Threshold')\n",
        "    plt.ylabel('Metric Score')\n",
        "    plt.legend(loc=\"lower left\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.xlim([0, 1])\n",
        "    plt.ylim([0, 1.05])\n",
        "\n",
        "    # Save for publication\n",
        "    plt.savefig('Precision_Recall_Threshold_Tradeoff.png', dpi=300, bbox_inches='tight')\n",
        "    plt.savefig('Precision_Recall_Threshold_Tradeoff.pdf', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "# Execute the plot\n",
        "plot_precision_recall_vs_threshold(all_labels, probs, optimal_t=0.85)"
      ],
      "metadata": {
        "id": "xjDi2XbdXcBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# gPPipred APP"
      ],
      "metadata": {
        "id": "HpOLXuYMKbs_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gradio as gr\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tempfile\n",
        "from torch_geometric.data import Data, Batch\n",
        "from torch_geometric.nn import GATv2Conv, global_mean_pool\n",
        "\n",
        "# --- 1. CONFIGURATION & PHYSICS ---\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "AA_PHYSICS_5D_SCALED = {\n",
        "    'A': [0.70, 0.17, 0.11, 0.40, 0.00], 'R': [0.00, 0.67, 0.71, 1.00, 1.00],\n",
        "    'N': [0.11, 0.32, 0.33, 0.33, 0.00], 'D': [0.11, 0.30, 0.26, 0.00, 0.31],\n",
        "    'C': [0.78, 0.29, 0.31, 0.29, 0.67], 'Q': [0.11, 0.50, 0.44, 0.36, 0.00],\n",
        "    'E': [0.11, 0.47, 0.37, 0.06, 0.34], 'G': [0.46, 0.00, 0.00, 0.40, 0.00],\n",
        "    'H': [0.14, 0.55, 0.56, 0.60, 0.48], 'I': [1.00, 0.64, 0.45, 0.41, 0.00],\n",
        "    'L': [0.92, 0.64, 0.45, 0.40, 0.00], 'K': [0.07, 0.65, 0.54, 0.87, 0.84],\n",
        "    'M': [0.66, 0.61, 0.54, 0.37, 0.00], 'F': [0.81, 0.77, 0.72, 0.34, 0.00],\n",
        "    'P': [0.32, 0.31, 0.32, 0.44, 0.00], 'S': [0.41, 0.17, 0.15, 0.36, 0.00],\n",
        "    'T': [0.42, 0.33, 0.26, 0.35, 0.00], 'W': [0.40, 1.00, 1.00, 0.39, 0.00],\n",
        "    'Y': [0.36, 0.79, 0.73, 0.36, 0.81], 'V': [0.97, 0.48, 0.34, 0.40, 0.00],\n",
        "    'X': [0.00, 0.00, 0.00, 0.00, 0.00]\n",
        "}\n",
        "\n",
        "# --- 2. ARCHITECTURE ---\n",
        "class SiameseGAT_v3(nn.Module):\n",
        "    def __init__(self, in_channels=5, hidden_channels=64, num_layers=8):\n",
        "        super(SiameseGAT_v3, self).__init__()\n",
        "        self.node_lin = nn.Linear(in_channels, hidden_channels)\n",
        "        self.convs = nn.ModuleList()\n",
        "        self.batch_norms = nn.ModuleList()\n",
        "        for _ in range(num_layers):\n",
        "            self.convs.append(GATv2Conv(hidden_channels, hidden_channels // 4, heads=4))\n",
        "            self.batch_norms.append(nn.BatchNorm1d(hidden_channels))\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(hidden_channels * 2, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "\n",
        "    def forward_once(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "        x = self.node_lin(x)\n",
        "        for conv, bn in zip(self.convs, self.batch_norms):\n",
        "            h = conv(x, edge_index)\n",
        "            h = bn(h)\n",
        "            h = F.elu(h)\n",
        "            x = x + h\n",
        "        return global_mean_pool(x, batch)\n",
        "\n",
        "    def forward(self, g1, g2):\n",
        "        out1 = self.forward_once(g1)\n",
        "        out2 = self.forward_once(g2)\n",
        "        combined = torch.cat([out1, out2], dim=1)\n",
        "        return self.fc(combined)\n",
        "\n",
        "# --- 3. HOTSPOT LOGIC ---\n",
        "def get_hotspots(model, g_bait, g_prey, bait_seq, prey_seq, top_n=10):\n",
        "    g_bait.x.requires_grad = True\n",
        "    g_prey.x.requires_grad = True\n",
        "    logits = model(Batch.from_data_list([g_bait]), Batch.from_data_list([g_prey]))\n",
        "    model.zero_grad()\n",
        "    logits.backward()\n",
        "\n",
        "    def extract_top(grad, seq):\n",
        "        importance = grad.abs().sum(dim=1).cpu().numpy()\n",
        "        indices = np.argsort(importance)[-top_n:][::-1]\n",
        "        return \", \".join([f\"{seq[i]}{i+1}\" for i in indices if i < len(seq)])\n",
        "\n",
        "    return extract_top(g_bait.x.grad, bait_seq), extract_top(g_prey.x.grad, prey_seq)\n",
        "\n",
        "# --- 4. DATA UTILS ---\n",
        "CACHE_DIR = \"protein_cache\"\n",
        "os.makedirs(CACHE_DIR, exist_ok=True)\n",
        "\n",
        "def get_protein_data(uniprot_id):\n",
        "    uniprot_id = uniprot_id.strip().upper()\n",
        "    cache_path = os.path.join(CACHE_DIR, f\"{uniprot_id}.pt\")\n",
        "    if os.path.exists(cache_path): return torch.load(cache_path)\n",
        "\n",
        "    # Fetch FASTA for sequence\n",
        "    res_fasta = requests.get(f\"https://rest.uniprot.org/uniprotkb/{uniprot_id}.fasta\")\n",
        "    # Fetch JSON for metadata (Protein Name)\n",
        "    res_json = requests.get(f\"https://rest.uniprot.org/uniprotkb/{uniprot_id}.json\")\n",
        "\n",
        "    if res_fasta.status_code != 200: raise ValueError(f\"ID {uniprot_id} not found.\")\n",
        "\n",
        "    seq = \"\".join(res_fasta.text.split(\"\\n\")[1:])\n",
        "\n",
        "    # Extract Protein Name\n",
        "    p_name = \"Unknown Protein\"\n",
        "    if res_json.status_code == 200:\n",
        "        json_data = res_json.json()\n",
        "        p_name = json_data.get('proteinDescription', {}).get('recommendedName', {}).get('fullName', {}).get('value', \"Unknown\")\n",
        "\n",
        "    L = len(seq)\n",
        "    adj = np.eye(L)\n",
        "    for i in range(L - 1):\n",
        "        adj[i, i+1] = 1; adj[i+1, i] = 1\n",
        "    edge_index = torch.from_numpy(adj).nonzero().t().contiguous().long()\n",
        "\n",
        "    x = torch.tensor([AA_PHYSICS_5D_SCALED.get(a, AA_PHYSICS_5D_SCALED['X']) for a in seq], dtype=torch.float32)\n",
        "    data = Data(x=x, edge_index=edge_index)\n",
        "\n",
        "    package = (data, seq, p_name)\n",
        "    torch.save(package, cache_path)\n",
        "    return package\n",
        "\n",
        "# --- 5. PREDICTION CORE ---\n",
        "def run_prediction(bait_id, prey_raw, threshold):\n",
        "    try:\n",
        "        prey_ids = [p.strip().upper() for p in prey_raw.replace(',', ' ').split() if p.strip()]\n",
        "        g_bait, bait_seq, bait_name = get_protein_data(bait_id)\n",
        "\n",
        "        results = []\n",
        "        for p_id in prey_ids:\n",
        "            g_prey, prey_seq, prey_name = get_protein_data(p_id)\n",
        "\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                logits = model(Batch.from_data_list([g_bait.to(DEVICE)]),\n",
        "                               Batch.from_data_list([g_prey.to(DEVICE)]))\n",
        "                prob = torch.sigmoid(logits).item()\n",
        "\n",
        "            b_hot, p_hot = get_hotspots(model, g_bait.to(DEVICE), g_prey.to(DEVICE), bait_seq, prey_seq)\n",
        "\n",
        "            results.append({\n",
        "                \"Bait ID\": bait_id,\n",
        "                \"Bait Name\": bait_name,\n",
        "                \"Prey ID\": p_id,\n",
        "                \"Prey Name\": prey_name,\n",
        "                \"Probability\": round(prob, 4),\n",
        "                \"Binds\": \"Yes\" if prob >= threshold else \"No\",\n",
        "                \"Bait Hotspots\": b_hot,\n",
        "                \"Prey Hotspots\": p_hot,\n",
        "                \"Bait Sequence\": bait_seq,\n",
        "                \"Prey Sequence\": prey_seq\n",
        "            })\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        df = pd.DataFrame(results)\n",
        "\n",
        "        # Plotting logic\n",
        "        plt.figure(figsize=(8, max(4, 0.5 * len(df))))\n",
        "        sns.barplot(data=df, x='Probability', y='Prey ID', palette='viridis')\n",
        "        plt.axvline(threshold, color='red', linestyle='--', label=f'Threshold {threshold}')\n",
        "        plt.title(f\"PPI Analysis for Bait: {bait_id}\")\n",
        "        plt.xlim(0, 1)\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "\n",
        "        tmp_csv = os.path.join(tempfile.gettempdir(), \"results.csv\")\n",
        "        df.to_csv(tmp_csv, index=False)\n",
        "        return plt.gcf(), df, tmp_csv, f\"‚úÖ Analyzed {len(df)} preys.\"\n",
        "    except Exception as e:\n",
        "        return None, None, None, f\"‚ùå Error: {str(e)}\"\n",
        "\n",
        "# --- 6. INITIALIZATION & UI ---\n",
        "model = SiameseGAT_v3(in_channels=5).to(DEVICE)\n",
        "MODEL_PATH = \"GATv3_FINAL_CORRECTED.pth\"\n",
        "if os.path.exists(MODEL_PATH):\n",
        "    model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE, weights_only=True))\n",
        "\n",
        "custom_theme = gr.themes.Soft(font=[gr.themes.GoogleFont(\"Inter\"), \"sans-serif\"])\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# üß¨ gPPIpred v3 ‚Äì Advanced Siamese GAT Predictor\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=2):\n",
        "            b_in = gr.Textbox(label=\"Bait ID (UniProt)\", value=\"P04637\")\n",
        "            p_in = gr.Textbox(label=\"Prey IDs (comma separated)\", value=\"O15169, P00519\")\n",
        "        with gr.Column(scale=1):\n",
        "            t_slider = gr.Slider(0.1, 0.95, value=0.90, step=0.01, label=\"Sensitivity Threshold\")\n",
        "            btn = gr.Button(\"üîç Predict\", variant=\"primary\")\n",
        "            status = gr.Textbox(label=\"Status\")\n",
        "\n",
        "    with gr.Tabs():\n",
        "        with gr.TabItem(\"Chart\"): plot = gr.Plot()\n",
        "        with gr.TabItem(\"Data\"):\n",
        "            table = gr.Dataframe()\n",
        "            csv_file = gr.File(label=\"Download CSV\")\n",
        "\n",
        "    btn.click(run_prediction, inputs=[b_in, p_in, t_slider], outputs=[plot, table, csv_file, status])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(theme=custom_theme)"
      ],
      "metadata": {
        "id": "Dy_z-TWvsGdI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}